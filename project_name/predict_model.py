import torch
from models.model import Model


def make_prediction(model, input_prompt, max_output_length):
    """

    Args:
        model: Model used for making predictions
        input_prompt (str): input prompt for the model, that will be used to generate text
        max_output_length (int): The maximum length for the

    Returns:
        prediction (str): input + text generated by the model in a string

    """

    indexed_tokens = model.tokenizer.encode(input_prompt)
    tokens_tensor = torch.tensor([indexed_tokens])
    prediction = model.generate(inputs=tokens_tensor, 
                                max_output_length=max_output_length,
                                num_return_sequences=1)

    return prediction[0]



if __name__ == '__main__':
    model = Model(model_version="data/processed/MLOps data")
    prompt = "What is the fastest car in the"
    predicted_text = make_prediction(model, prompt, 100)
    print(predicted_text)